# -*- coding: utf-8 -*-
"""FakeNewsDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IK926cs7CRmt5oz6aWWtTmfxXWWcru0l

##Data Processing

Removed rows with missing values to make sure the data is complete.

Removed HTML tags to clean up the text.

Converted all text to lowercase to keep things consistent.

Removed duplicate news articles to avoid repeated information in training.
"""

from google.colab import drive
drive.mount('/content/gdrive')
# connect to the google drive

cd/content/gdrive/MyDrive/AIstudio

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# read data
df = pd.read_csv("Fakenews_dataset.csv")

df.head()

# datatype info
df.info()

#Checking for null values

df.isnull()
#sum of null values
np.sum(df.isnull().any(axis=1))

print('length of data is', len(df))

"""# **Preprocessing**"""

#text  Cleaning
def clean_text_advanced(text):
    # Convert to lowercase
    text = text.lower()

    # Remove emojis
    text = emoji.replace_emoji(text, replace='')

    # Remove HTML tags
    text = BeautifulSoup(text, "html.parser").get_text()

    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)

    # Remove email addresses
    text = re.sub(r'\S+@\S+', '', text)

    # Remove special characters and digits
    text = re.sub(r"[^a-zA-Z\s]", '', text)

    # Remove extra whitespace
    text = re.sub(r"\s+", ' ', text).strip()

    return text

"""# **Tokenization, Lemmatization, and More Filters**"""

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess_text_advanced(text):
    # Tokenize and remove stopwords and short words
    tokens = [word for word in text.split() if word not in stop_words and len(word) > 2]

    # Lemmatize tokens
    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]

    return " ".join(lemmatized)

#Better
!pip install nltk

nltk.download('punkt_tab') # download 'punkt_tab'

lemmatizer = WordNetLemmatizer()
stpwrds = list(stopwords.words('english'))

for x in range(len(df)):
    corpus = []
    review = df['text'][x]
    review = re.sub(r'[^a-zA-Z\s]', '', review)
    review = review.lower()
    review = nltk.word_tokenize(review)  # Now, nltk should find the necessary resources
    for y in review:
        if y not in stpwrds:
            corpus.append(lemmatizer.lemmatize(y))
    review = ' '.join(corpus)
    df['text'][x] = review

import spacy
nlp = spacy.load("en_core_web_sm")

def remove_named_entities(text):
    doc = nlp(text)
    tokens = [token.text for token in doc if not token.ent_type_]
    return " ".join(tokens)

# Detect & Remove Repeated Characters

def reduce_repeated_characters(text):
    return re.sub(r'(.)\1{2,}', r'\1\1', text)

df = df.drop("subject", axis = 1)
df = df.drop("date", axis = 1)

df.head()

label_train = df.label

label_train.head(10)

df.head(10)

lemmatizer = WordNetLemmatizer()
stpwrds = list(stopwords.words('english'))

stpwrds

X_train, X_test, Y_train, Y_test = train_test_split(df['text'], label_train, test_size=0.3, random_state=1)

X_train

X_train.shape

Y_train

"""### Feture Extraction"""



"""# **Feature extraction**"""

tfidf_v = TfidfVectorizer()
tfidf_X_train = tfidf_v.fit_transform(X_train)
tfidf_X_test = tfidf_v.transform(X_test)

tfidf_X_train.shape

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
from sklearn.linear_model import PassiveAggressiveClassifier


classifier = PassiveAggressiveClassifier()
classifier.fit(tfidf_X_train,Y_train)

Y_pred = classifier.predict(tfidf_X_test)
score = metrics.accuracy_score(Y_test, Y_pred)
print(f'Accuracy: {round(score*100,2)}%')



from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
from sklearn.linear_model import PassiveAggressiveClassifier
import itertools # Import the itertools module


classifier = PassiveAggressiveClassifier()
classifier.fit(tfidf_X_train,Y_train)



def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')


Y_pred = classifier.predict(tfidf_X_test)
score = metrics.accuracy_score(Y_test, Y_pred)
print(f'Accuracy: {round(score*100,2)}%')



from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
cm = metrics.confusion_matrix(Y_test, Y_pred)
plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])

# Classification Report (Precision, Recall, F1)
#print(classification_report(Y_test, Y_pred))



# Binarize the labels
# Assuming Y_test contains 0 and 1  for fake and real
#Y_test_bin = label_binarize(Y_test, classes=[0, 1])

# Get predicted probabilities
# Assuming 'classifier' is your trained model
#Y_scores = classifier.decision_function(tfidf_X_test)


#fpr, tpr, _ = roc_curve(Y_test_bin, Y_scores)
#roc_auc = roc_auc_score(Y_test_bin, Y_scores)

# Now you can proceed to plot the ROC curve if needed...